{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials for training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data preparing in CNN-based super-resolution research is always based on one assumption: Lr image is the `bicubic` downsampled version of Hr image. However we support many kind of degradations such as `gaussian` kernel, `bilinear` kernel, additive noise and so on. \n",
    "\n",
    "- We usually crop images into patches for convenience of training, but reconstruct the whole image when testing (when full convolution net). Based on the fact that the order of downsampling and cropping operation doesn't matter, I suggest to crop the image and save patches to `tfrecord` file first, then use `map` method to downsample each hr-patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./Image/set14\" # Arbitrary\n",
    "valid_dir = \"./Image/set5\"\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "SCALE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.write2tfrec import write_dst_tfrec, load_tfrecord\n",
    "\n",
    "cache_dir = \"./cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving patch into tfrecord file. IF saved, we can use directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================WRITING TO TFRECORD=========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 39.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================WRITING TO TFRECORD=========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 42.13it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not os.path.isfile(\"./cache/set14_train_48x48.tfrec\"):\n",
    "    paths = list(glob.glob(os.path.join(train_dir, \"*\")))\n",
    "    write_dst_tfrec(paths, 10, 48, \"./cache/set14_train_48x48.tfrec\")\n",
    "    \n",
    "if not os.path.isfile(\"./cache/set5_valid_48x48.tfrec\"):\n",
    "    paths = list(glob.glob(os.path.join(valid_dir, \"*\")))\n",
    "    write_dst_tfrec(paths, 10, 48, \"./cache/set5_valid_48x48.tfrec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input should be hr-patch, and output should be data pair (inputs, labels).\n",
    "The degradation function used here is pre-defined in `preprocess.py` file, one can \n",
    "customize if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess import degrade_image\n",
    "\n",
    "def preprocess(hr):\n",
    "    lr, hr = degrade_image(hr, SCALE, method=-1, restore_shape=False, kernel_sigma=0.5)\n",
    "    return lr, hr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tfrecord file and map the preprocess function. \n",
    "`repeat()` makes the dataset repeat infinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdst = load_tfrecord(48, \"./cache/set14_train_48x48.tfrec\").map(preprocess).repeat()\n",
    "valdst = load_tfrecord(48, \"./cache/set5_valid_48x48.tfrec\").map(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a pre-defined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import EDSR_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train EDSR-baseline model for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EDSR_baseline(scale=SCALE, model_name=\"EDSR_Baseline\",\n",
    "                      channel=3).create_model(load_weights=False,\n",
    "                                              weights_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model : EDSR_Baseline_X3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0621 21:28:34.268955 14980 training_utils.py:1353] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
      "W0621 21:28:34.341790 14980 tf_logging.py:161] Model failed to serialize as JSON. Ignoring... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 1/20 [>.............................] - ETA: 2:33 - loss: 5.8379 - psnr_tf: -6.6579"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0621 21:28:42.690458 14980 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.202476). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/20 [==>...........................] - ETA: 1:15 - loss: 3.4289 - psnr_tf: -2.9330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0621 21:28:42.760271 14980 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.181550). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/20 [===========================>..] - ETA: 47s - loss: 2.5048 - psnr_tf: -0.9582 - ETA: 33s - loss: 1.9969 - psnr_tf: 0.303 - ETA: 25s - loss: 1.6836 - psnr_tf: 1.07 - ETA: 20s - loss: 1.4455 - psnr_tf: 1.96 - ETA: 16s - loss: 1.3108 - psnr_tf: 2.17 - ETA: 13s - loss: 1.1800 - psnr_tf: 2.67 - ETA: 10s - loss: 1.0655 - psnr_tf: 3.32 - ETA: 8s - loss: 0.9781 - psnr_tf: 3.7442 - ETA: 7s - loss: 0.9038 - psnr_tf: 4.137 - ETA: 5s - loss: 0.8438 - psnr_tf: 4.431 - ETA: 4s - loss: 0.7910 - psnr_tf: 4.727 - ETA: 3s - loss: 0.7501 - psnr_tf: 4.897 - ETA: 3s - loss: 0.7145 - psnr_tf: 5.049 - ETA: 2s - loss: 0.6785 - psnr_tf: 5.286 - ETA: 1s - loss: 0.6442 - psnr_tf: 5.591 - ETA: 1s - loss: 0.6162 - psnr_tf: 5.765 - ETA: 0s - loss: 0.5908 - psnr_tf: 5.9322\n",
      "Epoch 00001: saving model to ./weights/EDSR_Baseline_X3.h5\n",
      "20/20 [==============================] - 10s 511ms/step - loss: 0.5670 - psnr_tf: 6.1199 - val_loss: 0.1163 - val_psnr_tf: 9.6231\n",
      "Epoch 2/2\n",
      "19/20 [===========================>..] - ETA: 1s - loss: 0.0946 - psnr_tf: 10.63 - ETA: 1s - loss: 0.1453 - psnr_tf: 9.0496 - ETA: 1s - loss: 0.1401 - psnr_tf: 9.113 - ETA: 1s - loss: 0.1250 - psnr_tf: 9.642 - ETA: 0s - loss: 0.1228 - psnr_tf: 9.669 - ETA: 0s - loss: 0.1219 - psnr_tf: 9.643 - ETA: 0s - loss: 0.1210 - psnr_tf: 9.646 - ETA: 0s - loss: 0.1173 - psnr_tf: 9.771 - ETA: 0s - loss: 0.1200 - psnr_tf: 9.697 - ETA: 0s - loss: 0.1238 - psnr_tf: 9.581 - ETA: 0s - loss: 0.1221 - psnr_tf: 9.624 - ETA: 0s - loss: 0.1180 - psnr_tf: 9.786 - ETA: 0s - loss: 0.1184 - psnr_tf: 9.752 - ETA: 0s - loss: 0.1184 - psnr_tf: 9.731 - ETA: 0s - loss: 0.1170 - psnr_tf: 9.770 - ETA: 0s - loss: 0.1152 - psnr_tf: 9.842 - ETA: 0s - loss: 0.1190 - psnr_tf: 9.724 - ETA: 0s - loss: 0.1193 - psnr_tf: 9.707 - ETA: 0s - loss: 0.1169 - psnr_tf: 9.8027\n",
      "Epoch 00002: saving model to ./weights/EDSR_Baseline_X3.h5\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.1164 - psnr_tf: 9.8166 - val_loss: 0.1081 - val_psnr_tf: 9.9161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.model.EDSR.EDSR_baseline at 0x29b8b951550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trdst,\n",
    "          valdst,\n",
    "          nb_epochs=2,\n",
    "          steps_per_epoch=20,\n",
    "          batch_size=16,\n",
    "          use_wn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
