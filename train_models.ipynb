{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials for training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data preparing in CNN-based super-resolution research is always based on one assumption: Lr image is the `bicubic` downsampled version of Hr image. However we support many kind of degradations such as `gaussian` kernel, `bilinear` kernel, additive noise and so on. \n",
    "\n",
    "- We usually crop images into patches for convenience of training, but reconstruct the whole image when testing (when full convolution net). Based on the fact that the order of downsampling and cropping operation doesn't matter, I suggest to crop the image and save patches to `tfrecord` file first, then use `map` method to downsample each hr-patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./Image/set14\" # Arbitrary\n",
    "valid_dir = \"./Image/set5\"\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "SCALE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.write2tfrec import write_dst_tfrec, load_tfrecord\n",
    "\n",
    "cache_dir = \"./cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving patch into tfrecord file. IF saved, we can use directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================WRITING TO TFRECORD=========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 37.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================WRITING TO TFRECORD=========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 51.69it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not os.path.isfile(\"./cache/set14_train_48x48.tfrec\"):\n",
    "    write_dst_tfrec(train_dir, 10, 48, \"./cache/set14_train_48x48.tfrec\")\n",
    "    \n",
    "if not os.path.isfile(\"./cache/set5_valid_48x48.tfrec\"):\n",
    "    write_dst_tfrec(valid_dir, 10, 48, \"./cache/set5_valid_48x48.tfrec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input should be hr-patch, and output should be data pair (inputs, labels).\n",
    "The degradation function used here is pre-defined in `preprocess.py` file, one can \n",
    "customize if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess import degrade_image\n",
    "\n",
    "def preprocess(hr):\n",
    "    lr, hr = degrade_image(hr, SCALE, method=2, restore_shape=False)\n",
    "    return lr, hr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tfrecord file and map the preprocess function. \n",
    "`repeat()` makes the dataset repeat infinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdst = load_tfrecord(48, \"./cache/set14_train_48x48.tfrec\").map(preprocess,AUTOTUNE).repeat()\n",
    "valdst = load_tfrecord(48, \"./cache/set5_valid_48x48.tfrec\").map(preprocess, AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a pre-defined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import EDSR_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train EDSR-baseline model for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EDSR_baseline(scale=SCALE, model_name=\"EDSR_Baseline\",\n",
    "                      channel=3).create_model(load_weights=False,\n",
    "                                              weights_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model : EDSR_Baseline_X3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0619 16:30:54.574010 12428 tf_logging.py:161] Model failed to serialize as JSON. Ignoring... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "18/20 [==========================>...] - ETA: 2:14 - loss: 5.1590 - psnr_tf: -5.80 - ETA: 1:05 - loss: 2.9116 - psnr_tf: -1.86 - ETA: 41s - loss: 2.1425 - psnr_tf: -0.1534 - ETA: 29s - loss: 1.7002 - psnr_tf: 1.130 - ETA: 22s - loss: 1.4164 - psnr_tf: 2.14 - ETA: 17s - loss: 1.2251 - psnr_tf: 2.80 - ETA: 14s - loss: 1.0789 - psnr_tf: 3.47 - ETA: 11s - loss: 0.9694 - psnr_tf: 3.96 - ETA: 7s - loss: 0.8121 - psnr_tf: 4.7335 - ETA: 5s - loss: 0.7018 - psnr_tf: 5.371 - ETA: 3s - loss: 0.6248 - psnr_tf: 5.798 - ETA: 1s - loss: 0.5639 - psnr_tf: 6.226 - ETA: 0s - loss: 0.5143 - psnr_tf: 6.6263\n",
      "Epoch 00001: saving model to ./weights/EDSR_Baseline_X3.h5\n",
      "20/20 [==============================] - 9s 445ms/step - loss: 0.4752 - psnr_tf: 6.9275 - val_loss: 0.1055 - val_psnr_tf: 9.6688\n",
      "Epoch 2/2\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1136 - psnr_tf: 9.791 - ETA: 0s - loss: 0.1081 - psnr_tf: 10.18 - ETA: 0s - loss: 0.1093 - psnr_tf: 10.21 - ETA: 0s - loss: 0.1054 - psnr_tf: 10.33 - ETA: 0s - loss: 0.1052 - psnr_tf: 10.34 - ETA: 0s - loss: 0.1049 - psnr_tf: 10.32 - ETA: 0s - loss: 0.1061 - psnr_tf: 10.27 - ETA: 0s - loss: 0.1040 - psnr_tf: 10.38 - ETA: 0s - loss: 0.1050 - psnr_tf: 10.33 - ETA: 0s - loss: 0.1039 - psnr_tf: 10.38 - ETA: 0s - loss: 0.1037 - psnr_tf: 10.38 - ETA: 0s - loss: 0.1031 - psnr_tf: 10.4029\n",
      "Epoch 00002: saving model to ./weights/EDSR_Baseline_X3.h5\n",
      "20/20 [==============================] - 1s 69ms/step - loss: 0.1035 - psnr_tf: 10.3852 - val_loss: 0.0988 - val_psnr_tf: 9.9610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.model.EDSR.EDSR_baseline at 0x1798ea0fe10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trdst,\n",
    "          valdst,\n",
    "          nb_epochs=2,\n",
    "          steps_per_epoch=20,\n",
    "          batch_size=16,\n",
    "          use_wn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
